{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bonito-train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n20E4-WjmQ8J",
        "colab_type": "text"
      },
      "source": [
        "# Bonito Train\n",
        "![](https://github.com/nanoporetech/ont_tutorial_transcriptome/raw/master/Static/Images/ONT_logo.png)\n",
        "\n",
        "This notebook demonstrates the process of training a [basecaller](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1727-y) for [Oxford Nanopore reads](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-018-1462-9), following the method demonstrated in [bonito](https://github.com/nanoporetech/bonito). This basecaller leverages NVIDIA's recently published [QuartzNet](https://arxiv.org/pdf/1910.10261.pdf) convolutional neural network architecture.\n",
        "\n",
        "A modestly sized dataset is provided however it is not anticipated that models trained using these data exclusively will be production quality. The notbook serves as a learning resource only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDijiEYaKOPw",
        "colab_type": "text"
      },
      "source": [
        "#### Install/import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJNyEwpkFEjv",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "!pip install -q ont-bonito\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "import os\n",
        "import sys\n",
	"import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "from itertools import starmap\n",
        "\n",
        "from google.colab import auth\n",
        "from google.colab import drive as gdrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import toml\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from bonito.model import Model\n",
	"from bonito.util import accuracy\n",
        "from bonito.training import ChunkDataSet\n",
	"from bonito.decode import decode, decode_ref"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaravkLeSj7U",
        "colab_type": "text"
      },
      "source": [
        "#### Set up data import functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOUFthMnSbCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticate and create PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# helper functions for importing data\n",
        "def download_npy_from_link(fn, link):\n",
        "    _, id = link.split('=')\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile(fn)\n",
        "    return np.load(fn)\n",
        "\n",
        "def download_toml_from_link(fn, link):\n",
        "    _, id = link.split('=')\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile(fn)\n",
        "    return toml.load(fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZb5SQfuJl1T",
        "colab_type": "text"
      },
      "source": [
        "#### Specify training data\n",
        "Required data for 100,000 signal 'chunks' will be loaded from a Google Drive in the form of 4 numpy files.\n",
        "- chunks.npy\n",
        "- chunk_lengths.npy\n",
        "- references.npy\n",
        "- reference_lengths.npy\n",
        "\n",
        "If you have an alternative set of four matching files in a Google Drive, you can overwrite the sharable links below. See [here](https://github.com/nanoporetech/bonito/issues/4) for a description of those files)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY6ZsGYEk7Cr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "chunks_link = \"https://drive.google.com/open?id=1aciNfQs53eFRwnMggInY-Uisi-owtmzY\" #@param {type:\"string\"}\n",
        "chunk_lengths_link = \"https://drive.google.com/open?id=1sW31y1IdkqjyWb9WNSun7iLRznJQ3hfs\" #@param {type:\"string\"}\n",
        "references_link = \"https://drive.google.com/open?id=1kcs_hZMndUIDX2n8dTxGrAgCvt_TpUcH\" #@param {type:\"string\"}\n",
        "reference_lengths_link = \"https://drive.google.com/open?id=1-r7XymddP_3gKFb-7ohB_t14u7u4SGLm\" #@param {type:\"string\"}\n",
        "quartznet_config_link = \"https://drive.google.com/open?id=1mLqxHMYKA4vfK9wd_2YgaBGzWPVBeilI\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9gjDpVCSr3w",
        "colab_type": "text"
      },
      "source": [
        "#### Import training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDCtusFznuIo",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "print('Loading chunks.')\n",
        "full_chunks = download_npy_from_link('chunks.npy',\n",
        "                                chunks_link)\n",
        "# Sections of squiggle that correspond with the target reference sequence\n",
        "# Variable length and zero padded (upto 4096 samples).\n",
        "# shape (1000000, 4096)\n",
        "# dtype('float32')\n",
        "\n",
        "print('Loading chunk lengths.')\n",
        "full_chunk_lengths = download_npy_from_link('chunk_lengths.npy',\n",
        "                                       chunk_lengths_link)\n",
        "# Lengths of squiggle sections in chunks.npy \n",
        "# shape (1000000,)\n",
        "# dtype('uint16')\n",
        "\n",
        "print('Loading references.')\n",
        "full_targets = download_npy_from_link('references.npy',\n",
        "                                 references_link)\n",
        "# Integer encoded target sequence {'A': 1, 'C': 2, 'G': 3, 'T': 4}\n",
        "# Variable length and zero padded (default range between 128 and 256).\n",
        "# shape (1000000, 256)\n",
        "# dtype('uint8')\n",
        "\n",
        "print('Loading reference lengths.')\n",
        "full_target_lengths = download_npy_from_link('reference_lengths.npy',\n",
        "                                        reference_lengths_link)\n",
        "# Lengths of target sequences in references.npy\n",
        "# shape (1000000,)\n",
        "# dtype('uint8')\n",
        "\n",
        "print('Loading quartznet config.')\n",
        "quartznet_config = download_toml_from_link(\"config_quartznet5x5.toml\",\n",
        "                                           quartznet_config_link)\n",
        "# The structure of the model is defined using a config file.\n",
        "# This will make sense to those familar with QuartzNet\n",
        "# https://arxiv.org/pdf/1910.10261.pdf)."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xXDEw3TLqnd",
        "colab_type": "text"
      },
      "source": [
        "#### Training options\n",
        "Default options are set, and ranges are sensible, but most combinations of settings are untested.\n",
        "\n",
        "The default settings will train on a small amount of data (1000 signal chunks) for a small number of epochs (20). This is unlikely to produce an accurate generalisable model, but will train relatively quickly.\n",
        "\n",
        "After modifying this cell, Runtime -> Run after, so that all cells between this one and the main train looping will be run in accordance with new setting.\n",
        "\n",
        "A train_proportion of 0.90 will use 90% of the data for training and 10% for validation.\n",
        "\n",
        "No dropout is applied by default, but in order to avoid overfitting on small data sets it may be necessary to apply dropout (e.g. of 0.5), or other regularisation techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xdd2e2Vufdc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "model_savepath = '/content/drive/My Drive/nitobook/' #@param {type:\"string\"}\n",
        "learning_rate = 0.001 #@param {type:\"number\"}\n",
        "random_seed = 25 #@param {type:\"integer\"}\n",
        "epochs = 20 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
        "batch_size = 16 #@param [2, 4, 8, 16, 28] {type:\"raw\"}\n",
        "num_chunks = 10000 #@param [10, 100, 1000, 10000, 100000] {type:\"raw\"}\n",
        "train_proportion = 0.80 #@param type:\"slider\", min:0.8, max:1000, step:1\n",
        "dropout = 0.0 #@param {type:\"slider\", min:0.0, max:0.8}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU1rLEyXCn9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init(random_seed):\n",
        "    \"\"\"Initialise random libs and setup cudnn\n",
        " \n",
        "    https://pytorch.org/docs/stable/notes/randomness.html\n",
        "    \"\"\"\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# set random seeds and setup pytorch\n",
        "init(random_seed)\n",
        "\n",
        "# we exploit GPU for training\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1INM1AeFRZcQ",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare data according to values set in the 'Training options'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPXMFHvyauaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# subset\n",
        "chunks = full_chunks[:num_chunks]\n",
        "chunk_lengths = full_chunk_lengths[:num_chunks]\n",
        "targets = full_targets[:num_chunks]\n",
        "target_lengths = full_target_lengths[:num_chunks]\n",
        "\n",
        "# shuffle\n",
        "shuf = np.random.permutation(chunks.shape[0])\n",
        "chunks = chunks[shuf]\n",
        "chunk_lengths = chunk_lengths[shuf]\n",
        "targets = targets[shuf]\n",
        "target_lengths = target_lengths[shuf]\n",
        "\n",
        "split = np.floor(chunks.shape[0] * train_proportion).astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvVR3YWLDr3H",
        "colab_type": "text"
      },
      "source": [
        "Modify quartznet config to set dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4047uqhD0uZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for b in quartznet_config['block']:\n",
        "    b['dropout'] = dropout\n",
        "quartznet_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plc7whByTKN6",
        "colab_type": "text"
      },
      "source": [
        "#### Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYjC-nPr8mkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 'Connectionist Temporal Classification' (CTC) loss fuction\n",
        "# https://distill.pub/2017/ctc/\n",
        "criterion = nn.CTCLoss(reduction='mean')\n",
        "\n",
        "def train(log_interval, model, device, train_loader,\n",
        "          optimizer, epoch, use_amp=False):\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    chunks = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    sys.stderr.write(\"\\n\" + \"Training epoch: \" + str(epoch) + \"\\n\")\n",
        "    progress_bar = tqdm(total=len(train_loader), leave=True, ncols=100)\n",
        "\n",
        "    for batch_idx, (data, out_lengths, target, lengths) in enumerate(train_loader, start=1):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        chunks += data.shape[0]\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        log_probs = model(data)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = criterion(log_probs.transpose(0, 1), target, out_lengths / model.stride, lengths)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "        progress_bar.refresh()\n",
        "        progress_bar.update(1)\n",
        "        progress_bar.set_description(\"Loss: \" + str(loss.item()))\n",
        "        sys.stderr.flush()        \n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    return loss.item(), time.perf_counter() - t0\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    predictions = []\n",
        "    prediction_lengths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, out_lengths, target, lengths) in enumerate(test_loader, start=1):\n",
        "            data, target = data.to(device), target.to(device)\n",
        " \n",
        "            # forward pass\n",
        "            log_probs = model(data)\n",
        " \n",
        "            # calculate loss\n",
        "            test_loss += criterion(log_probs.transpose(1, 0), target, out_lengths / model.stride, lengths)\n",
        "\n",
        "            # accumulate output probabilities\n",
        "            predictions.append(torch.exp(log_probs).cpu())\n",
        "            prediction_lengths.append(out_lengths / model.stride)\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    lengths = np.concatenate(prediction_lengths)\n",
        "\n",
        "    # convert probabilities to sequences\n",
        "    references = [decode_ref(target, model.alphabet) for target in test_loader.dataset.targets]\n",
        "    sequences = [decode(post[:n], model.alphabet) for post, n in zip(predictions, lengths)]\n",
        "\n",
        "    # align predicted sequences with true sequences and calculate accuracy\n",
        "    if all(map(len, sequences)):\n",
        "        accuracies = list(starmap(accuracy, zip(references, sequences)))\n",
        "    else:\n",
        "        accuracies = [0]\n",
        "\n",
        "    # measure average accuracies over entire set of validation chunks\n",
        "    mean = np.mean(accuracies)\n",
        "    median = np.median(accuracies)\n",
        "\n",
        "    return test_loss.item() / batch_idx, mean, median"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9wxTCwZoWkR",
        "colab_type": "text"
      },
      "source": [
        "#### Main training loop\n",
        "When this cell is run we begin training the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc0tIFsZ9oLU",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Set experiment name\n",
        "experiment_name = 'bonito_training_321' #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLnfmQw4kxmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount users drive to save data\n",
        "gdrive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# prevent overwriting of data\n",
        "workdir = os.path.join(model_savepath, experiment_name)\n",
        "if os.path.isdir(workdir):\n",
        "    raise IOError('{} already exists. Select an alternative model_savepath.'.format(workdir))\n",
        "os.makedirs(workdir)\n",
        "\n",
        "# data generators\n",
        "train_dataset = ChunkDataSet(chunks[:split], chunk_lengths[:split],\n",
        "                             targets[:split], target_lengths[:split])\n",
        "test_dataset = ChunkDataSet(chunks[split:], chunk_lengths[split:],\n",
        "                            targets[split:], target_lengths[split:])\n",
        "\n",
        "# data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                         num_workers=4, pin_memory=True)\n",
        "\n",
        "# load bonito model\n",
        "model = Model(quartznet_config)\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "# set optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), amsgrad=True, lr=learning_rate)\n",
        "schedular = CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
        "\n",
        "# report loss every \n",
        "interval = 500 / num_chunks\n",
        "log_interval = np.floor(len(train_dataset) / batch_size * interval)\n",
        "\n",
        "exp_config = os.path.join(workdir, \"experimental.log\")\n",
        "with open(exp_config, 'a') as c:\n",
        "    c.write('Num training chunks: {}'.format(num_chunks) + '\\n')\n",
        "    c.write('learning rate: {}'.format(learning_rate) + '\\n')\n",
        "    c.write('random seed: {}'.format(random_seed) + '\\n')\n",
        "    c.write('epochs: {}'.format(epochs) + '\\n')\n",
        "    c.write('batch_size: {}'.format(batch_size) + '\\n')\n",
        "    c.write('train proportion: {}'.format(train_proportion) + '\\n')\n",
        "    c.write('dropout: {}'.format(dropout) + '\\n')\n",
        "\n",
        "# DataFrame to store training logging information\n",
        "training_results = pd.DataFrame()\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "\n",
        "    train_loss, duration = train(log_interval, model, device,\n",
        "                                 train_loader, optimizer, epoch)\n",
        "    \n",
        "    test_loss, mean, median = test(model, device, test_loader)\n",
        "\n",
        "    # collate training and validation metrics\n",
        "    epoch_result = pd.DataFrame(\n",
        "        {'time':[datetime.today()],\n",
        "         'duration':[int(duration)],\n",
        "         'epoch':[epoch],\n",
        "         'train_loss':[train_loss],\n",
        "         'validation_loss':[test_loss], \n",
        "         'validation_mean':[mean],\n",
        "         'validation_median':[median]})\n",
        "    \n",
        "    # save model weights\n",
        "    weights_path = os.path.join(workdir, \"weights_%s.tar\" % epoch)\n",
        "    torch.save(model.state_dict(), weights_path)\n",
        "\n",
        "    # update log file\n",
        "    log_path = os.path.join(workdir, \"training.log\")\n",
        "    epoch_result.to_csv(log_path, mode='a', sep='\\t', index=False)\n",
        "\n",
        "    display(epoch_result)\n",
        "    training_results = training_results.append(epoch_result)\n",
        "\n",
        "    schedular.step()\n",
        "\n",
        "display(training_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhJu0WgU7NyD",
        "colab_type": "text"
      },
      "source": [
        "Complete training log will be displayed upon training completion.\n",
        "\n",
        "Partial logs are saved to the model_savepath."
      ]
    }
  ]
}